{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of stable_baselines_getting_started.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robertmoni/modelbasedrl/blob/master/Copy_of_stable_baselines_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2",
        "colab_type": "text"
      },
      "source": [
        "# Stable Baselines, a Fork of OpenAI Baselines - Getting Started\n",
        "\n",
        "Github Repo: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)\n",
        "\n",
        "Medium article: [https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82](https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82)\n",
        "\n",
        "## Install Dependencies and Stable Baselines Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/hill-a/stable-baselines).\n",
        "\n",
        "```\n",
        "\n",
        "sudo apt-get update && sudo apt-get install cmake libopenmpi-dev zlib1g-dev\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "pip install stable-baselines\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "3a9c8536-a910-4ee7-ee58-5b281c65b4c6"
      },
      "source": [
        "!pip install stable-baselines==2.6.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stable-baselines==2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/1e/f99bb18f6a24a88fdd528561c204ed82d644f83dbf7da4950900d6920dc5/stable_baselines-2.6.0-py3-none-any.whl (250kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.6.0) (1.16.4)\n",
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.6.0) (3.0.2)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.6.0) (0.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.6.0) (0.13.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.6.0) (3.4.5.20)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.6.0) (0.24.2)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.10.9 in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.6.0) (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.6.0) (3.0.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines==2.6.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines==2.6.0) (2.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines==2.6.0) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines==2.6.0) (2.21.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines==2.6.0) (1.4.1)\n",
            "Requirement already satisfied: PyOpenGL; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines==2.6.0) (3.1.0)\n",
            "Requirement already satisfied: atari-py>=0.1.4; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines==2.6.0) (0.1.15)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines==2.6.0) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines==2.6.0) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines==2.6.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines==2.6.0) (1.1.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari,classic_control]>=0.10.9->stable-baselines==2.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari,classic_control]>=0.10.9->stable-baselines==2.6.0) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari,classic_control]>=0.10.9->stable-baselines==2.6.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari,classic_control]>=0.10.9->stable-baselines==2.6.0) (2.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari,classic_control]>=0.10.9->stable-baselines==2.6.0) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow; extra == \"atari\"->gym[atari,classic_control]>=0.10.9->stable-baselines==2.6.0) (0.46)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->stable-baselines==2.6.0) (41.0.1)\n",
            "Installing collected packages: stable-baselines\n",
            "  Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm",
        "colab_type": "text"
      },
      "source": [
        "## Import policy, RL agent, ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.ppo2 import PPO2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd",
        "colab_type": "text"
      },
      "source": [
        "## Create the Gym env and instantiate the agent\n",
        "\n",
        "For this example, we will use CartPole environment, a classic control problem.\n",
        "\n",
        "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
        "\n",
        "Cartpole environment: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)\n",
        "\n",
        "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
        "\n",
        "Note: vectorized environments allow to easily multiprocess training. In this example, we are using only one process, hence the DummyVecEnv.\n",
        "\n",
        "We chose the MlpPolicy because input of CartPole is a feature vector, not images.\n",
        "\n",
        "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUWGZp3i9wyf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "ee181d87-c13b-4aa4-ad7c-97e01b726283"
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "# vectorized environments allow to easily multiprocess training\n",
        "# we demonstrate its usefulness in the next examples\n",
        "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n",
        "\n",
        "model = PPO2(MlpPolicy, env, verbose=0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0722 21:55:38.644183 140512076314496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:98: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0722 21:55:38.645871 140512076314496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:107: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0722 21:55:38.674685 140512076314496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:114: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0722 21:55:38.676453 140512076314496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0722 21:55:38.725634 140512076314496 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W0722 21:55:38.990430 140512076314496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/a2c/utils.py:156: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0722 21:55:39.501094 140512076314496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:193: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0722 21:55:39.544506 140512076314496 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0722 21:55:39.832764 140512076314496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:209: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0722 21:55:40.180288 140512076314496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:245: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efFdrQ7MBvl",
        "colab_type": "text"
      },
      "source": [
        "We create a helper function to evaluate the agent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63M8mSKR-6Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, num_steps=1000):\n",
        "  \"\"\"\n",
        "  Evaluate a RL agent\n",
        "  :param model: (BaseRLModel object) the RL Agent\n",
        "  :param num_steps: (int) number of timesteps to evaluate it\n",
        "  :return: (float) Mean reward for the last 100 episodes\n",
        "  \"\"\"\n",
        "  episode_rewards = [0.0]\n",
        "  obs = env.reset()\n",
        "  for i in range(num_steps):\n",
        "      # _states are only useful when using LSTM policies\n",
        "      action, _states = model.predict(obs)\n",
        "      # here, action, rewards and dones are arrays\n",
        "      # because we are using vectorized env\n",
        "      obs, rewards, dones, info = env.step(action)\n",
        "      \n",
        "      # Stats\n",
        "      episode_rewards[-1] += rewards[0]\n",
        "      if dones[0]:\n",
        "          obs = env.reset()\n",
        "          episode_rewards.append(0.0)\n",
        "  # Compute mean reward for the last 100 episodes\n",
        "  mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n",
        "  print(\"Mean reward:\", mean_100ep_reward, \"Num episodes:\", len(episode_rewards))\n",
        "  \n",
        "  return mean_100ep_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate the un-trained agent, this should be a random agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDHLMA6NFk95",
        "colab_type": "code",
        "outputId": "3dc744e3-2595-4d02-bcd8-e6ad97ba4279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Random Agent, before training\n",
        "mean_reward_before_train = evaluate(model, num_steps=10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean reward: 22.8 Num episodes: 453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE",
        "colab_type": "text"
      },
      "source": [
        "## Train the agent and evaluate it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4cfSXIB-pTF",
        "colab_type": "code",
        "outputId": "88ad61ef-d0c6-4f3b-fdaa-6fd12a3b310a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Train the agent for 10000 steps\n",
        "model.learn(total_timesteps=10000)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines.ppo2.ppo2.PPO2 at 0x7fcb353730b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygl_gVmV_QP7",
        "colab_type": "code",
        "outputId": "2f070af8-573a-490d-d73f-fe29369a4aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward = evaluate(model, num_steps=10000)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean reward: 166.7 Num episodes: 60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG",
        "colab_type": "text"
      },
      "source": [
        "Apparently the training went well, the mean reward increased a lot ! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y8zg4V566qD",
        "colab_type": "text"
      },
      "source": [
        "## Bonus: Train a RL Model in One Line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaOPfOrwWEP4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "00f45f80-19d4-4b0f-945b-63ccc30ca302"
      },
      "source": [
        "model = PPO2('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating environment from the given name, wrapped in a DummyVecEnv.\n",
            "--------------------------------------\n",
            "| approxkl           | 4.4630156e-07 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | 0.0192        |\n",
            "| fps                | 400           |\n",
            "| n_updates          | 1             |\n",
            "| policy_entropy     | 0.69314665    |\n",
            "| policy_loss        | 2.0517851e-05 |\n",
            "| serial_timesteps   | 128           |\n",
            "| time_elapsed       | 3.1e-06       |\n",
            "| total_timesteps    | 128           |\n",
            "| value_loss         | 37.039703     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 3.612032e-05   |\n",
            "| clipfrac           | 0.0            |\n",
            "| explained_variance | -0.011         |\n",
            "| fps                | 1094           |\n",
            "| n_updates          | 2              |\n",
            "| policy_entropy     | 0.6931189      |\n",
            "| policy_loss        | -0.00081435114 |\n",
            "| serial_timesteps   | 256            |\n",
            "| time_elapsed       | 0.321          |\n",
            "| total_timesteps    | 256            |\n",
            "| value_loss         | 45.310932      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 4.3759588e-05  |\n",
            "| clipfrac           | 0.0            |\n",
            "| explained_variance | 0.0172         |\n",
            "| fps                | 1108           |\n",
            "| n_updates          | 3              |\n",
            "| policy_entropy     | 0.69289166     |\n",
            "| policy_loss        | -0.00044666952 |\n",
            "| serial_timesteps   | 384            |\n",
            "| time_elapsed       | 0.439          |\n",
            "| total_timesteps    | 384            |\n",
            "| value_loss         | 39.413483      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 1.2264762e-05  |\n",
            "| clipfrac           | 0.0            |\n",
            "| explained_variance | -0.00943       |\n",
            "| fps                | 1131           |\n",
            "| n_updates          | 4              |\n",
            "| policy_entropy     | 0.69275415     |\n",
            "| policy_loss        | -0.00082016527 |\n",
            "| serial_timesteps   | 512            |\n",
            "| time_elapsed       | 0.556          |\n",
            "| total_timesteps    | 512            |\n",
            "| value_loss         | 56.089584      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 3.943781e-05   |\n",
            "| clipfrac           | 0.0            |\n",
            "| explained_variance | 0.0614         |\n",
            "| fps                | 1064           |\n",
            "| n_updates          | 5              |\n",
            "| policy_entropy     | 0.6920338      |\n",
            "| policy_loss        | -0.00086386444 |\n",
            "| serial_timesteps   | 640            |\n",
            "| time_elapsed       | 0.67           |\n",
            "| total_timesteps    | 640            |\n",
            "| value_loss         | 53.73777       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 8.066463e-05 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | 0.0293       |\n",
            "| fps                | 1135         |\n",
            "| n_updates          | 6            |\n",
            "| policy_entropy     | 0.6904164    |\n",
            "| policy_loss        | -0.001097451 |\n",
            "| serial_timesteps   | 768          |\n",
            "| time_elapsed       | 0.791        |\n",
            "| total_timesteps    | 768          |\n",
            "| value_loss         | 31.952532    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 8.348669e-05  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | 0.00736       |\n",
            "| fps                | 1095          |\n",
            "| n_updates          | 7             |\n",
            "| policy_entropy     | 0.6887087     |\n",
            "| policy_loss        | -0.0011623823 |\n",
            "| serial_timesteps   | 896           |\n",
            "| time_elapsed       | 0.905         |\n",
            "| total_timesteps    | 896           |\n",
            "| value_loss         | 30.444292     |\n",
            "--------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se8SHBN17Fy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}